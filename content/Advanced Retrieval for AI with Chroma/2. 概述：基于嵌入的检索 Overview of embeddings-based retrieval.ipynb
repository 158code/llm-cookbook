{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68dd396-943d-493a-bb4d-16ac9ec6609f",
   "metadata": {},
   "source": [
    "# 第二章 概述：基于嵌入的检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "- [ 一、课程notebook注意事项](#31)\n",
    "- [ 二、课程内容](#32)\n",
    "- [ 2.1 系统运行原理](#33)\n",
    "- [ 2.2 系统具体实现](#34)\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd9a78-cd9c-4dae-a91b-08cf2a0a9df0",
   "metadata": {},
   "source": [
    "## 一、课程notebook注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1646079-661e-4fd6-8b94-b1d4e7cd9159",
   "metadata": {},
   "source": [
    "- 在notebook运行的过程中，可能会弹出大量的warning。这是正常现象且并不影响后续结果，可以忽略。\n",
    "- 部分操作（如调用LLM或使用生成的数据集）可能产生不可预测的返回结果，因此输出结果可能和视频中不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e96532-3105-4957-90f5-dc4f67e29c71",
   "metadata": {},
   "source": [
    "## 二、课程内容\n",
    "第一节课中，我们将回顾嵌入式检索系统中的一些元素，以及它们如何在一个检索增强的生成循环中与一个大型语言模型（LLM）一起配合使用。\n",
    "### 2.1 系统运作原理\n",
    "在Chorma的案例中，检索增强的方式是，当一个用户查询请求进入时，已经有运作嵌入并存储在检索系统中的文档。\n",
    "当接受到请求时，通过用有相同嵌入的模型运行该请求，来生成嵌入。\n",
    "当查询请求被嵌入时，检索系统就会根据该查询的嵌入通过最近邻的方法，找到最相关的文档。\n",
    "最后把查询请求和相关文档一起交给LLM， LLM从检索到的文档中的综合信息来生成答案。\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef011ec-79ff-44bc-af8d-61116e0e12b5",
   "metadata": {},
   "source": [
    "## 2.2 系统具体实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf548c-1fa0-4974-b73d-077f8d3f30a4",
   "metadata": {},
   "source": [
    "首先，从工具库中引入一些辅助函数。helper_utils.py文件可在当前目录中找到。\n",
    "该函数是一个基础的自动换行函数，它能够以一种美观、整洁的方式查看文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0873df-123e-4c09-b258-eaf75be7da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_utils import word_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f41e63-c787-41f9-8006-2c0caba405f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PDF阅读器\n",
    "from pypdf import PdfReader\n",
    "# 使用microsoft_annual_report_2022作为示例文件\n",
    "reader = PdfReader (\"microsoft_annual_report_2022.pdf\")\n",
    "# 从该文件中提取文本，并跳过空格\n",
    "pdf_texts=[p.extract_text().strip() for p in reader.pages]\n",
    "\n",
    "# 过滤空行，因为检索系统不能接受空行\n",
    "pdf_texts=[text for text in pdf_texts if text]\n",
    "\n",
    "print(word_wrap(pdf_texts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc1b6c-b728-4445-8708-2fd418d18ce2",
   "metadata": {},
   "source": [
    "如果想查看该pdf文件的话，请在data目录里查找。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e748b-6a60-462d-962e-b1c6255e7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在LangChain工具集中，使用递归字符文本拆分器和句子转换器令牌文本拆分器。\n",
    "# 字符拆分器可以根据特定的分隔符递归地划分文本，使得它可以在文本中查找指定的字符并在这些字符处将文本呢分割成更小的片段。\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19909d-de1f-4f7b-a580-10d7e56ff9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15590844-49ec-4c16-8f19-c49f9122d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用的嵌入模型称为句子转换器，对上下文窗口宽度有限制，最大时256个字符。\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba90112-8646-4952-bd17-85439f34c649",
   "metadata": {},
   "source": [
    "这里一个小陷阱。如果你不习惯处理嵌入，你可能不会考虑嵌入模型上下文窗口本身，\n",
    "但这非常重要，因为通常一个嵌入模型有一个固定的上下文窗口大小，这意味着它在任何给定时间只能考虑一定数量的词。\n",
    "这个上下文窗口限制了模型能够“看到”和因此处理的文本长度。\n",
    "如果文本超过了模型的上下文窗口大小，模型可能无法捕捉到超出窗口范围的文本信息，这可能会影响嵌入的质量和最终的检索或生成结果的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130219d8-d48d-42e7-9009-8d04a31eb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用BERT来实现句子转换器\n",
    "# 句子转换器是出色的嵌入模型，内置于Chorma中，开源且所有权重可在线获取。\n",
    "# 下面的工作是为了创建一个句子转换器嵌入函数，使其能够和Chorma一起使用。\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "print(embedding_function([token_split_texts[10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640096b7-9d60-49b5-a696-a5fc3de4089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来是设置Chroma\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"microsoft_annual_report_2022\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ced5f-e137-4305-818d-57ecb113f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在所有内容都加载到了Chorma中，让我们连接一个LLM并构建一个完整的检索增强生成（RAG)系统\n",
    "# 接下来演示查询、检索和LLM是如何一起工作的\n",
    "query = \"What was the total revenue?\"\n",
    "\n",
    "# 查询Chorma来获取结果，请求5个结果\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(word_wrap(document))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa50ca-9ffb-42e1-8d94-ca73afb263fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来，将这些结果与LLM一起使用，来回答查询\n",
    "# 使GPT进行操作，以便拥有一个OpenAI客户端\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25668232-f943-4f4a-809f-b0ec70da8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPT3.5 Turbo完成后续操作\n",
    "def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report.\"\n",
    "            \"You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c7300-c141-4096-892b-8124a3adb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(word_wrap(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
