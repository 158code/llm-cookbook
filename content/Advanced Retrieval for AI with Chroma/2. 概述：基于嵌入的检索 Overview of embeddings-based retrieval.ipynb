{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68dd396-943d-493a-bb4d-16ac9ec6609f",
   "metadata": {},
   "source": [
    "# 2. 概述：基于嵌入的检索\n",
    "第一节课中，我们将回顾嵌入式检索系统中的一些元素，以及它们如何在一个检索增强的生成循环中与一个大型语言模型（LLM）一起配合使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164d820",
   "metadata": {},
   "source": [
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#一课程notebook注意事项\" data-toc-modified-id=\"一、课程notebook注意事项\">一、课程notebook注意事项</a></span></li>\n",
    "        <li>\n",
    "        <span><a href=\"#二课程内容\" data-toc-modified-id=\"二、课程内容\">二、课程内容</a></span></li><li>\n",
    "        <ul class=\"toc-item\">\n",
    "            <li><span><a href=\"#21-系统运作原理\" data-toc-modified-id=\"2.1 系统运作原理\">2.1 系统运作原理</a></span></li>\n",
    "            <li><span><a href=\"#22-系统具体实现\" data-toc-modified-id=\"2.2 系统具体实现\">2.2 系统具体实现</a></span></li>\n",
    "        </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd9a78-cd9c-4dae-a91b-08cf2a0a9df0",
   "metadata": {},
   "source": [
    "## 一、课程notebook注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1646079-661e-4fd6-8b94-b1d4e7cd9159",
   "metadata": {},
   "source": [
    "- 在notebook运行的过程中，可能会弹出大量的warning。这是正常现象且并不影响后续结果，可以忽略。\n",
    "- 部分操作（如调用LLM或使用生成的数据集）可能产生不可预测的返回结果，因此输出结果可能和视频中不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e96532-3105-4957-90f5-dc4f67e29c71",
   "metadata": {},
   "source": [
    "## 二、课程内容\n",
    "### 2.1 系统运作原理\n",
    "在Chorma的案例中，检索增强的方式是，当一个用户查询请求进入时，已经有运作嵌入并存储在检索系统中的文档。\n",
    "当接受到请求时，通过用有相同嵌入的模型运行该请求，来生成嵌入。\n",
    "当查询请求被嵌入时，检索系统就会根据该查询的嵌入通过最近邻的方法，找到最相关的文档。\n",
    "最后把查询请求和相关文档一起交给LLM， LLM从检索到的文档中的综合信息来生成答案。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef011ec-79ff-44bc-af8d-61116e0e12b5",
   "metadata": {},
   "source": [
    "### 2.2 系统具体实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf548c-1fa0-4974-b73d-077f8d3f30a4",
   "metadata": {},
   "source": [
    "首先，从工具库中引入一些辅助函数。helper_utils.py文件可在当前目录中找到。\n",
    "该函数是一个基础的自动换行函数，它能够以一种美观、整洁的方式查看文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0873df-123e-4c09-b258-eaf75be7da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_utils import word_wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f41e63-c787-41f9-8006-2c0caba405f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Dear shareholders, colleagues, customers, and partners:  \n",
      "We are\n",
      "living through a period of historic economic, societal, and\n",
      "geopolitical change. The world in 2022 looks nothing like \n",
      "the world in\n",
      "2019. As I write this, inflation is at a 40 -year high, supply chains\n",
      "are stretched, and the war in Ukraine is \n",
      "ongoing. At the same time, we\n",
      "are entering a technological era with the potential to power awesome\n",
      "advancements \n",
      "across every sector of our economy and society. As the\n",
      "world’s largest software company, this places us at a historic\n",
      "\n",
      "intersection of opportunity and responsibility to the world around us.\n",
      " \n",
      "Our mission to empower every person and every organization on the\n",
      "planet to achieve more has never been more \n",
      "urgent or more necessary.\n",
      "For all the uncertainty in the world, one thing is clear: People and\n",
      "organizations in every \n",
      "industry are increasingly looking to digital\n",
      "technology to overcome today’s challenges and emerge stronger. And no\n",
      "\n",
      "company is better positioned to help them than Microsoft.  \n",
      "Every day\n",
      "this past fiscal year I have had the privilege to witness our customers\n",
      "use our platforms and tools to connect \n",
      "what technology can do with\n",
      "what the world needs  it to do.  \n",
      "Here are just a few examples:  \n",
      "•\n",
      "Ferrovial, which builds and manages some of the world’s busiest\n",
      "airports and highways, is using our cloud \n",
      "infrastructure to build\n",
      "safer roads as it prepares for a future of autonomous transportation. \n",
      "\n",
      "• Peace Parks Foundation, a nonprofit helping protect natural\n",
      "ecosystems in Southern Africa, is using Microsoft \n",
      "Dynamics 365 and\n",
      "Power BI to secure essential funding, as well as our Azure AI and IoT\n",
      "solutions to help \n",
      "rangers scale their park maintenance and wildlife\n",
      "crime prevention work.  \n",
      "• One of the world’s largest robotics\n",
      "companies, Kawasaki Heavy Industries, is using the breadth of our tools\n",
      "—\n",
      "from Azure IoT and HoloLens —to create an industrial metaverse\n",
      "solution that brings its distributed workforce \n",
      "together with its\n",
      "network of connected equipment to improve productivity and keep\n",
      "employees safe.  \n",
      "• Globo, the biggest media and TV company in Brazil,\n",
      "is using Power Platform to empower its employees to \n",
      "build their own\n",
      "solutions for everything from booking sets to setting schedules.  \n",
      "•\n",
      "And Ørsted, which produces a quarter of the world’s wind energy, is\n",
      "using the Microsoft Intelligent Data \n",
      "Platform to turn data from its\n",
      "offshore turbines into insights for predictive maintenance.  \n",
      "Amid this\n",
      "dynamic environment, we delivered record results in fiscal year 2022:\n",
      "We reported $198  billion in revenue and \n",
      "$83 billion in operating\n",
      "income. And the Microsoft Cloud surpassed $100  billion in annualized\n",
      "revenue for the first time.  \n",
      "OUR RESPONSIBILITY  \n",
      "As a corporation,\n",
      "our purpose and actions must be aligned with addressing the world’s\n",
      "problems, not creating new ones. \n",
      "At our very core, we need to deliver\n",
      "innovation that helps drive broad economic growth. We, as a company,\n",
      "will do well \n",
      "when the world around us does well.  \n",
      "That’s what I\n",
      "believe will lead to widespread human progress and ultimately improve\n",
      "the lives of everyone. There is no \n",
      "more powerful input than digital\n",
      "technology to drive the world’s economic output. This is the core\n",
      "thesis for our being as a \n",
      "company, but it’s not enough. As we drive\n",
      "global economic growth, we must also commit to creating a more\n",
      "inclusive, \n",
      "equitable, sustainable, and trusted future.  \n",
      "Support\n",
      "inclusive economic growth  \n",
      "We must ensure the growth we drive reaches\n",
      "every person, organization, community, and country. This starts with\n",
      "\n",
      "increasing access to digital skills. This year alone, more than 23 \n",
      "million people accessed digital skills training as part of \n",
      "our global\n",
      "skills initiative.\n"
     ]
    }
   ],
   "source": [
    "# 导入PDF阅读器\n",
    "from pypdf import PdfReader\n",
    "# 使用microsoft_annual_report_2022作为示例文件\n",
    "reader = PdfReader(\"./data/microsoft_annual_report_2022.pdf\")\n",
    "# 从该文件中提取文本，并跳过空格\n",
    "pdf_texts=[p.extract_text().strip() for p in reader.pages]\n",
    "\n",
    "# 过滤空行，因为检索系统不能接受空行\n",
    "pdf_texts=[text for text in pdf_texts if text]\n",
    "\n",
    "print(word_wrap(pdf_texts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc1b6c-b728-4445-8708-2fd418d18ce2",
   "metadata": {},
   "source": [
    "如果想查看该pdf文件的话，请在data目录里查找。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e748b-6a60-462d-962e-b1c6255e7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在LangChain工具集中，使用递归字符文本拆分器和句子转换器令牌文本拆分器。\n",
    "# 字符拆分器可以根据特定的分隔符递归地划分文本，使得它可以在文本中查找指定的字符并在这些字符处将文本呢分割成更小的片段。\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19909d-de1f-4f7b-a580-10d7e56ff9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(word_wrap(character_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15590844-49ec-4c16-8f19-c49f9122d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用的嵌入模型称为句子转换器，对上下文窗口宽度有限制，最大时256个字符。\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(word_wrap(token_split_texts[10]))\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba90112-8646-4952-bd17-85439f34c649",
   "metadata": {},
   "source": [
    "这里一个小陷阱。如果你不习惯处理嵌入，你可能不会考虑嵌入模型上下文窗口本身，\n",
    "但这非常重要，因为通常一个嵌入模型有一个固定的上下文窗口大小，这意味着它在任何给定时间只能考虑一定数量的词。\n",
    "这个上下文窗口限制了模型能够“看到”和因此处理的文本长度。\n",
    "如果文本超过了模型的上下文窗口大小，模型可能无法捕捉到超出窗口范围的文本信息，这可能会影响嵌入的质量和最终的检索或生成结果的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130219d8-d48d-42e7-9009-8d04a31eb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用BERT来实现句子转换器\n",
    "# 句子转换器是出色的嵌入模型，内置于Chorma中，开源且所有权重可在线获取。\n",
    "# 下面的工作是为了创建一个句子转换器嵌入函数，使其能够和Chorma一起使用。\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "print(embedding_function([token_split_texts[10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640096b7-9d60-49b5-a696-a5fc3de4089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来是设置Chroma\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"microsoft_annual_report_2022\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=token_split_texts)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ced5f-e137-4305-818d-57ecb113f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在所有内容都加载到了Chorma中，让我们连接一个LLM并构建一个完整的检索增强生成（RAG)系统\n",
    "# 接下来演示查询、检索和LLM是如何一起工作的\n",
    "query = \"What was the total revenue?\"\n",
    "\n",
    "# 查询Chorma来获取结果，请求5个结果\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(word_wrap(document))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa50ca-9ffb-42e1-8d94-ca73afb263fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来，将这些结果与LLM一起使用，来回答查询\n",
    "# 使GPT进行操作，以便拥有一个OpenAI客户端\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25668232-f943-4f4a-809f-b0ec70da8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPT3.5 Turbo完成后续操作\n",
    "def rag(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful expert financial research assistant. Your users are asking questions about information contained in an annual report.\"\n",
    "            \"You will be shown the user's question, and the relevant information from the annual report. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c7300-c141-4096-892b-8124a3adb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(word_wrap(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
